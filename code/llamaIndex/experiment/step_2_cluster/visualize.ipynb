{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# ------------------ Data Loading Functions ------------------\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    embeddings = []\n",
    "    levels = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            # Parse each line as JSON and append it to the list\n",
    "            obj = json.loads(line)\n",
    "            embeddings.append(obj['embedding'])\n",
    "            levels.append(obj['level'])\n",
    "    return embeddings, levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../step_1_get_embedding_value/contexts/embeddings_gpt-4o-batch-all-target_1_parser_ManuallyHierarchicalNodeParser_7652_gpu_V100_nodeNum_50_pid_1.jsonl\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import itertools\n",
    "# Load embeddings\n",
    "all_embeddings = []\n",
    "all_levels = []\n",
    "\n",
    "files = os.listdir(os.path.abspath('../step_1_get_embedding_value/contexts'))\n",
    "files = sorted(files, key=lambda x: int(x.split('.')[0].split('_')[-1]))\n",
    "\n",
    "for file_name in files[:10]:\n",
    "    embeddings_file_path = f\"../step_1_get_embedding_value/contexts/{file_name}\"\n",
    "    print(embeddings_file_path)\n",
    "    embeddings, levels = load_jsonl(embeddings_file_path)\n",
    "    \n",
    "    # Append to the combined lists\n",
    "    all_embeddings.append(embeddings)\n",
    "    all_levels.append(levels)\n",
    "\n",
    "# Combine all_embeddings and all_levels into a single list of tuples for easy random selection\n",
    "num_samples = 4\n",
    "selected_indices = random.sample(range(len(all_embeddings)), num_samples)\n",
    "print(selected_indices)\n",
    "selected_indices = [1,4,8,9]\n",
    "\n",
    "# Randomly select 4 tuples\n",
    "selected_embeddings = [all_embeddings[i] for i in selected_indices]\n",
    "selected_levels = [all_levels[i] for i in selected_indices]\n",
    "\n",
    "selected_embeddings = list(itertools.chain.from_iterable(selected_embeddings))\n",
    "selected_levels = list(itertools.chain.from_iterable(selected_levels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "# Load embeddings\n",
    "def load_4_sample_dataset():\n",
    "    all_embeddings = []\n",
    "    all_levels = []\n",
    "\n",
    "    files = os.listdir(os.path.abspath('../step_1_get_embedding_value/contexts'))\n",
    "    files = sorted(files, key=lambda x: int(x.split('.')[0].split('_')[-1]))\n",
    "\n",
    "    for file_name in files[:10]:\n",
    "        embeddings_file_path = f\"../step_1_get_embedding_value/contexts/{file_name}\"\n",
    "        print(embeddings_file_path)\n",
    "        embeddings, levels = load_jsonl(embeddings_file_path)\n",
    "        \n",
    "        # Append to the combined lists\n",
    "        all_embeddings.append(embeddings)\n",
    "        all_levels.append(levels)\n",
    "\n",
    "    df_total = pd.DataFrame({\n",
    "        'embeddings': [tuple(embedding) for embedding in selected_embeddings],\n",
    "        'levels': selected_levels\n",
    "    })\n",
    "    return df_total\n",
    "df_total = load_4_sample_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading gpt-4o-batch-all-target_1_parser_ManuallyHierarchicalNodeParser_7652_gpu_V100_nodeNum_50_pid_0.jsonl: 100%|██████████| 144M/144M [00:03<00:00, 40.2MB/s] \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('../..'))\n",
    "from component.io import load_nodes_jsonl\n",
    "\n",
    "cache_file_path = \"../../.cache/gpt-4o-batch-all-target_1_parser_ManuallyHierarchicalNodeParser_7652_gpu_V100_nodeNum_50_pid_0.jsonl\"\n",
    "    \n",
    "nodes = load_nodes_jsonl(cache_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anytree import Node, RenderTree\n",
    "\n",
    "def load_5_documents_nodes():\n",
    "    # Assuming 'nodes' is a list of nodes that each have metadata like 'level' and 'ref_doc_id'\n",
    "    # and 'id_' which is an identifier of the node.\n",
    "    level2nodes = {'document': []}\n",
    "\n",
    "    # Build a dictionary to store nodes at different levels\n",
    "    for node in nodes:\n",
    "        level = node.metadata['level']\n",
    "        if level == 'document':\n",
    "            level2nodes[level].append(node)\n",
    "        else:\n",
    "            if level not in level2nodes:\n",
    "                level2nodes[level] = {}\n",
    "            if node.ref_doc_id not in level2nodes[level]:\n",
    "                level2nodes[level][node.ref_doc_id] = []\n",
    "            level2nodes[level][node.ref_doc_id].append(node)\n",
    "\n",
    "    # Get the top-level nodes (root) to build the tree\n",
    "    target_ids = [node.id_ for node in level2nodes['document'][:5]]\n",
    "    target_nodes = level2nodes['document'][:5]\n",
    "\n",
    "    # Create a root node for visualization\n",
    "    root = Node(\"Root\")  # this is the root node of the entire structure\n",
    "\n",
    "    # Dictionary to store anytree Node objects for reference\n",
    "    tree_nodes = {}\n",
    "\n",
    "    # Create nodes in the tree for the 'document' level\n",
    "    for doc_node in target_nodes:\n",
    "        tree_nodes[doc_node.id_] = Node(f\"Document {doc_node.id_}\", parent=root)\n",
    "\n",
    "    # Traverse the remaining levels and create child nodes\n",
    "    for level in level2nodes.keys():\n",
    "        if level == 'document':\n",
    "            continue\n",
    "        new_target_ids = []\n",
    "        for target_id in target_ids:\n",
    "            new_target_nodes = level2nodes[level][target_id]\n",
    "            for new_node in new_target_nodes:\n",
    "                tree_nodes[new_node.id_] = Node(f\"{level} Node {new_node.id_}\", parent=tree_nodes[target_id])\n",
    "            new_target_ids.extend([node.id_ for node in new_target_nodes])\n",
    "            target_nodes.extend(new_target_nodes)\n",
    "        target_ids = new_target_ids\n",
    "\n",
    "    # # Print the tree structure\n",
    "    # for pre, fill, node in RenderTree(root):\n",
    "    #     print(f\"{pre}{node.name}\")\n",
    "nodes = load_5_documents_nodes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "\n",
    "# Load embeddings\n",
    "def load_5_documents():\n",
    "    embeddings = []\n",
    "    levels = []\n",
    "    \n",
    "    \n",
    "\n",
    "    files = os.listdir(os.path.abspath('../step_1_get_embedding_value/contexts'))\n",
    "    files = sorted(files, key=lambda x: int(x.split('.')[0].split('_')[-1]))\n",
    "\n",
    "    file_name = files[0]\n",
    "    embeddings_file_path = f\"../step_1_get_embedding_value/contexts/{file_name}\"\n",
    "    print(embeddings_file_path)\n",
    "    embeddings, levels = load_jsonl(embeddings_file_path)\n",
    "\n",
    "    df_total = pd.DataFrame({\n",
    "        'embeddings': [tuple(embedding) for embedding in selected_embeddings],\n",
    "        'levels': selected_levels\n",
    "    })\n",
    "    return df_total\n",
    "df_total = load_4_sample_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'embeddings': [tuple(embedding) for embedding in all_embeddings[0]],\n",
    "    'levels': all_levels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if the value are unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document number of df: 50\n",
      "Unique embeddings counts:\n",
      "Document: 50\n",
      "Section: 959\n",
      "Paragraph: 3477\n",
      "Multi-Sentences: 8212\n"
     ]
    }
   ],
   "source": [
    "def unique_values(filtered_df):\n",
    "    # Count unique tsne values for each level\n",
    "    doc_count = len(filtered_df[filtered_df['levels'] == 'document']['embeddings'].unique())\n",
    "    section_count = len(filtered_df[filtered_df['levels'] == 'section']['embeddings'].unique())\n",
    "    paragraph_count = len(filtered_df[filtered_df['levels'] == 'paragraph']['embeddings'].unique())\n",
    "    multi_sent_count = len(filtered_df[filtered_df['levels'] == 'multi-sentences']['embeddings'].unique())\n",
    "\n",
    "    # Print formatted results\n",
    "    print(f\"Document number of df: {len(filtered_df[filtered_df['levels'] == 'document'])}\")\n",
    "    print(f\"Unique embeddings counts:\")\n",
    "    print(f\"Document: {doc_count}\")\n",
    "    print(f\"Section: {section_count}\")\n",
    "    print(f\"Paragraph: {paragraph_count}\")\n",
    "    print(f\"Multi-Sentences: {multi_sent_count}\")\n",
    "    \n",
    "unique_values(df_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def unique_values(filtered_df, output_file):\n",
    "    # Count unique tsne values for each level\n",
    "    unique_embeddings = {\n",
    "        'document': filtered_df[filtered_df['levels'] == 'document']['embeddings'].unique().tolist(),\n",
    "        'section': filtered_df[filtered_df['levels'] == 'section']['embeddings'].unique().tolist(),\n",
    "        'paragraph': filtered_df[filtered_df['levels'] == 'paragraph']['embeddings'].unique().tolist(),\n",
    "        'multi-sentences': filtered_df[filtered_df['levels'] == 'multi-sentences']['embeddings'].unique().tolist(),\n",
    "    }\n",
    "\n",
    "    # Print formatted results\n",
    "    print(f\"Unique embeddings counts:\")\n",
    "    for level, embeddings in unique_embeddings.items():\n",
    "        print(f\"{level.capitalize()}: {len(embeddings)}\")\n",
    "\n",
    "    # Save unique embeddings to JSONL file\n",
    "    with open(output_file, 'w') as f:\n",
    "        for level, embeddings in unique_embeddings.items():\n",
    "            for embedding in embeddings:\n",
    "                json_record = {'level': level, 'embedding': embedding}\n",
    "                f.write(json.dumps(json_record) + '\\n')\n",
    "\n",
    "# Usage\n",
    "unique_values(df_total, 'unique_embeddings.jsonl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate TSNE results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ TSNE and Data Preparation ------------------\n",
    "\n",
    "def get_tsne_result(embeddings, n_components=2, perplexity=30, random_state=42):\n",
    "    \"\"\"Generate TSNE result from embeddings.\"\"\"\n",
    "    tsne = TSNE(n_components, perplexity=perplexity, random_state=random_state)\n",
    "    tsne_result = tsne.fit_transform(embeddings)\n",
    "    return tsne_result\n",
    "\n",
    "def create_tsne_dataframe(tsne_result, levels):\n",
    "    \"\"\"Create a DataFrame with TSNE result and corresponding labels.\"\"\"\n",
    "    return pd.DataFrame({'tsne_1': tsne_result[:, 0], 'tsne_2': tsne_result[:, 1], 'label': levels})\n",
    "\n",
    "# ------------------ Plotting Functions ------------------\n",
    "\n",
    "def plot_tsne_result(tsne_result_df, unique_labels, palette, nodes_file_name): #nodes_file_name\n",
    "    \"\"\"Plot TSNE result using a scatter plot with subplots for individual labels.\"\"\"\n",
    "    # Create a figure and a GridSpec layout\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "    gs = GridSpec(2, 3, width_ratios=[2, 1, 1])\n",
    "\n",
    "    # Create the large plot on the left (spanning both rows)\n",
    "    ax_main = fig.add_subplot(gs[:, 0])\n",
    "    sns.scatterplot(data=tsne_result_df, x='tsne_1', y='tsne_2', hue='label', palette=palette, ax=ax_main, alpha=0.5)\n",
    "    ax_main.set_title('All Data')\n",
    "\n",
    "    # Get x and y axis limits from the main plot\n",
    "    xlim = ax_main.get_xlim()\n",
    "    ylim = ax_main.get_ylim()\n",
    "\n",
    "    # Create the 2x2 grid on the right for individual label plots\n",
    "    for i, label in enumerate(unique_labels[:4]):\n",
    "        ax = fig.add_subplot(gs[i // 2, i % 2 + 1])\n",
    "        label_data = tsne_result_df[tsne_result_df['label'] == label]\n",
    "        sns.scatterplot(data=label_data, x='tsne_1', y='tsne_2', color=palette[i], ax=ax, alpha=0.8)\n",
    "\n",
    "        # Set x and y limits to match the main plot\n",
    "        ax.set_xlim(xlim)\n",
    "        ax.set_ylim(ylim)\n",
    "\n",
    "        ax.set_title(f'Label {label}')\n",
    "        ax.legend([label])\n",
    "\n",
    "    # # Adjust layout and save the image\n",
    "    # plt.tight_layout()\n",
    "    \n",
    "    # Enable interactive mode\n",
    "    plt.ion()  # Use this in scripts to enable interactive mode\n",
    "    plt.show()  # Show the plot\n",
    "    plt.savefig(f\"tsne_result_{nodes_file_name}.png\")\n",
    "    # plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate TSNE result\n",
    "tsne_results = []\n",
    "embeddings = np.array(df_total['embeddings'].tolist())\n",
    "levels = df_total['levels'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_states = [42] # , 300, 1001\n",
    "perplexities = [10] # , 20, 30, 40, 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_state: 42\n",
      "property: 10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m perplexity \u001b[38;5;129;01min\u001b[39;00m perplexities: \u001b[38;5;66;03m# , 20, 30, 40, 50\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproperty: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mperplexity\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m     tsne_one_row\u001b[38;5;241m.\u001b[39mappend(\u001b[43mget_tsne_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperplexity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      8\u001b[0m tsne_results\u001b[38;5;241m.\u001b[39mappend(tsne_one_row)\n",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m, in \u001b[0;36mget_tsne_result\u001b[0;34m(embeddings, n_components, perplexity, random_state)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate TSNE result from embeddings.\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m tsne \u001b[38;5;241m=\u001b[39m TSNE(n_components, perplexity\u001b[38;5;241m=\u001b[39mperplexity, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m----> 6\u001b[0m tsne_result \u001b[38;5;241m=\u001b[39m \u001b[43mtsne\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tsne_result\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/sklearn/utils/_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    319\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/sklearn/manifold/_t_sne.py:1176\u001b[0m, in \u001b[0;36mTSNE.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params_vs_input(X)\n\u001b[0;32m-> 1176\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_ \u001b[38;5;241m=\u001b[39m embedding\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/sklearn/manifold/_t_sne.py:1044\u001b[0m, in \u001b[0;36mTSNE._fit\u001b[0;34m(self, X, skip_num_points)\u001b[0m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;66;03m# Degrees of freedom of the Student's t-distribution. The suggestion\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;66;03m# degrees_of_freedom = n_components - 1 comes from\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;66;03m# \"Learning a Parametric Embedding by Preserving Local Structure\"\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;66;03m# Laurens van der Maaten, 2009.\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m degrees_of_freedom \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 1044\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tsne\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m    \u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdegrees_of_freedom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_embedded\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_embedded\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mneighbors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneighbors_nn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_num_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_num_points\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/sklearn/manifold/_t_sne.py:1096\u001b[0m, in \u001b[0;36mTSNE._tsne\u001b[0;34m(self, P, degrees_of_freedom, n_samples, X_embedded, neighbors, skip_num_points)\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;66;03m# Learning schedule (part 1): do 250 iteration with lower momentum but\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;66;03m# higher learning rate controlled via the early exaggeration parameter\u001b[39;00m\n\u001b[1;32m   1095\u001b[0m P \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mearly_exaggeration\n\u001b[0;32m-> 1096\u001b[0m params, kl_divergence, it \u001b[38;5;241m=\u001b[39m \u001b[43m_gradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopt_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m   1099\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[t-SNE] KL divergence after \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m iterations with early exaggeration: \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1100\u001b[0m         \u001b[38;5;241m%\u001b[39m (it \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, kl_divergence)\n\u001b[1;32m   1101\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/sklearn/manifold/_t_sne.py:403\u001b[0m, in \u001b[0;36m_gradient_descent\u001b[0;34m(objective, p0, it, max_iter, n_iter_check, n_iter_without_progress, momentum, learning_rate, min_gain, min_grad_norm, verbose, args, kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;66;03m# only compute the error when needed\u001b[39;00m\n\u001b[1;32m    401\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompute_error\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m check_convergence \u001b[38;5;129;01mor\u001b[39;00m i \u001b[38;5;241m==\u001b[39m max_iter \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 403\u001b[0m error, grad \u001b[38;5;241m=\u001b[39m \u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m inc \u001b[38;5;241m=\u001b[39m update \u001b[38;5;241m*\u001b[39m grad \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    406\u001b[0m dec \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39minvert(inc)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/sklearn/manifold/_t_sne.py:284\u001b[0m, in \u001b[0;36m_kl_divergence_bh\u001b[0;34m(params, P, degrees_of_freedom, n_samples, n_components, angle, skip_num_points, verbose, compute_error, num_threads)\u001b[0m\n\u001b[1;32m    281\u001b[0m indptr \u001b[38;5;241m=\u001b[39m P\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint64, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    283\u001b[0m grad \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(X_embedded\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m--> 284\u001b[0m error \u001b[38;5;241m=\u001b[39m \u001b[43m_barnes_hut_tsne\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_P\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_embedded\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mneighbors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mangle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdof\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdegrees_of_freedom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m*\u001b[39m (degrees_of_freedom \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1.0\u001b[39m) \u001b[38;5;241m/\u001b[39m degrees_of_freedom\n\u001b[1;32m    298\u001b[0m grad \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39mravel()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Generate tsne with different parameters\n",
    "for random_state in random_states: # , 300, 1001\n",
    "    print(f\"random_state: {random_state}\")\n",
    "    tsne_one_row = []\n",
    "    for perplexity in perplexities: # , 20, 30, 40, 50\n",
    "        print(f\"property: {perplexity}\")\n",
    "        tsne_one_row.append(get_tsne_result(embeddings, 2, perplexity, random_state))\n",
    "    tsne_results.append(tsne_one_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tsne_result():\n",
    "    data_to_save = []\n",
    "\n",
    "    # Iterate over the random_state and properties\n",
    "    for i, random_state in enumerate(random_states): # , 300, 1001\n",
    "        for j, property in enumerate(perplexities): # , 20, 30, 40, 50\n",
    "            # Get the t-SNE results for this combination\n",
    "            tsne_result = tsne_results[i][j]\n",
    "            \n",
    "            # For each embedding row, append the random_state, property, levels, and t-SNE coordinates\n",
    "            for idx in range(len(levels)):\n",
    "                data_to_save.append([random_state, property, levels[idx], tsne_result[idx][0], tsne_result[idx][1]])\n",
    "\n",
    "    # Create a Pandas DataFrame from the collected data\n",
    "    df = pd.DataFrame(data_to_save, columns=['random_state', 'perplexity', 'level', 'tsne_x', 'tsne_y'])\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv('tsne_results_with_levels.csv', index=False)\n",
    "save_tsne_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def load_tsne_result(file_path='tsne_results_with_levels.csv'):\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['tsne'] = df.apply(lambda row: (row['tsne_x'], row['tsne_y']), axis=1)\n",
    "\n",
    "    \n",
    "    # Optional: Convert levels or other columns to categorical types if needed\n",
    "    df['level'] = df['level'].astype('category')\n",
    "    \n",
    "    return df\n",
    "df = load_tsne_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_values(filtered_df):\n",
    "    # Count unique tsne values for each level\n",
    "    doc_count = len(filtered_df[filtered_df['level'] == 'document']['tsne'].unique())\n",
    "    section_count = len(filtered_df[filtered_df['level'] == 'section']['tsne'].unique())\n",
    "    paragraph_count = len(filtered_df[filtered_df['level'] == 'paragraph']['tsne'].unique())\n",
    "    multi_sent_count = len(filtered_df[filtered_df['level'] == 'multi-sentences']['tsne'].unique())\n",
    "\n",
    "    # Print formatted results\n",
    "    print(f\"Unique tsne counts:\")\n",
    "    print(f\"Document: {doc_count}\")\n",
    "    print(f\"Section: {section_count}\")\n",
    "    print(f\"Paragraph: {paragraph_count}\")\n",
    "    print(f\"Multi-Sentences: {multi_sent_count}\")\n",
    "\n",
    "\n",
    "for r in df['random_state'].unique():\n",
    "    for p in df['perplexity'].unique():\n",
    "        print(f\"random state: {random_state}   propelxity: {p}\")\n",
    "        filtered_df = df[(df['random_state'] == r) & (df['perplexity'] == p)]\n",
    "        unique_values(filtered_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with TSNE result and labels\n",
    "tsne_result_df = create_tsne_dataframe(filtered_df[['tsne_x', 'tsne_y']].to_numpy(), filtered_df['level'])\n",
    "\n",
    "# Define unique labels and color palette\n",
    "unique_labels = tsne_result_df['label'].unique()\n",
    "palette = sns.color_palette('Set1', len(unique_labels))\n",
    "\n",
    "# Plot and save the TSNE result\n",
    "plot_tsne_result(\n",
    "    tsne_result_df=tsne_result_df, \n",
    "    unique_labels=unique_labels, \n",
    "    palette=palette, \n",
    "    nodes_file_name=\"tsne_5_sample\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TSNE - queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# ------------------ Data Loading Functions ------------------\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    embeddings = []\n",
    "    levels = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            # Parse each line as JSON and append it to the list\n",
    "            obj = json.loads(line)\n",
    "            embeddings.append(obj['embedding'])\n",
    "            levels.append(obj['level'])\n",
    "    return np.array(embeddings), levels\n",
    "\n",
    "# ------------------ TSNE and Data Preparation ------------------\n",
    "\n",
    "def get_tsne_result(embeddings, n_components=2):\n",
    "    \"\"\"Generate TSNE result from embeddings.\"\"\"\n",
    "    tsne = TSNE(n_components)\n",
    "    tsne_result = tsne.fit_transform(embeddings)\n",
    "    return tsne_result\n",
    "\n",
    "def create_tsne_dataframe(tsne_result, levels):\n",
    "    \"\"\"Create a DataFrame with TSNE result and corresponding labels.\"\"\"\n",
    "    return pd.DataFrame({'tsne_1': tsne_result[:, 0], 'tsne_2': tsne_result[:, 1], 'label': levels})\n",
    "\n",
    "# ------------------ Plotting Functions ------------------\n",
    "\n",
    "def plot_tsne_result(tsne_result_df, unique_labels, palette, nodes_file_name):\n",
    "    \"\"\"Plot TSNE result using a scatter plot with subplots for individual labels.\"\"\"\n",
    "    # Create a figure and a GridSpec layout\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "    gs = GridSpec(2, 3, width_ratios=[2, 1, 1])\n",
    "\n",
    "    # Create the large plot on the left (spanning both rows)\n",
    "    ax_main = fig.add_subplot(gs[:, 0])\n",
    "    sns.scatterplot(data=tsne_result_df, x='tsne_1', y='tsne_2', hue='label', palette=palette, ax=ax_main, alpha=0.5)\n",
    "    ax_main.set_title('All Data')\n",
    "\n",
    "    # Get x and y axis limits from the main plot\n",
    "    xlim = ax_main.get_xlim()\n",
    "    ylim = ax_main.get_ylim()\n",
    "\n",
    "    # Create the 2x2 grid on the right for individual label plots\n",
    "    for i, label in enumerate(unique_labels[:4]):\n",
    "        ax = fig.add_subplot(gs[i // 2, i % 2 + 1])\n",
    "        label_data = tsne_result_df[tsne_result_df['label'] == label]\n",
    "        sns.scatterplot(data=label_data, x='tsne_1', y='tsne_2', color=palette[i], ax=ax, alpha=0.8)\n",
    "\n",
    "        # Set x and y limits to match the main plot\n",
    "        ax.set_xlim(xlim)\n",
    "        ax.set_ylim(ylim)\n",
    "\n",
    "        ax.set_title(f'Label {label}')\n",
    "        ax.legend([label])\n",
    "\n",
    "    # Adjust layout and save the image\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"tsne_result_{nodes_file_name}.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid_num = 1\n",
    "# Load embeddings\n",
    "embeddings_file_path = f\"../1_get_embedding_value/data/embeddings_question_pid_{pid_num}.jsonl\"\n",
    "embeddings, levels = load_jsonl(embeddings_file_path)\n",
    "\n",
    "# Generate TSNE result\n",
    "tsne_result = get_tsne_result(embeddings, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with TSNE result and labels\n",
    "tsne_result_df = create_tsne_dataframe(tsne_result, levels)\n",
    "\n",
    "# Define unique labels and color palette\n",
    "unique_labels = tsne_result_df['label'].unique()\n",
    "palette = sns.color_palette('Set1', len(unique_labels))\n",
    "\n",
    "# Plot and save the TSNE result\n",
    "plot_tsne_result(tsne_result_df, unique_labels, palette, embeddings_file_path.split('/')[-1].split('.')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV file into DataFrame\n",
    "df_pca = pd.read_hdf(\"cluster_result_pca.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ae = pd.read_hdf(\"cluster_result_ae.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kmeans = pd.read_hdf(\"cluster_result_kmeans.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.insert(0, os.path.abspath('.'))\n",
    "from custom.io import load_nodes_jsonl\n",
    "\n",
    "def load_nodes(pid_num, cache_dir):\n",
    "    print(\"Loading nodes\")\n",
    "    file_name = f\"gpt-4o-batch-all-p_2_parser_ManuallyHierarchicalNodeParser_8165_gpu_V100_nodeNum_200_pid_{pid_num}.jsonl\"\n",
    "    file_path = os.path.join(cache_dir, file_name)\n",
    "    nodes = load_nodes_jsonl(file_path)\n",
    "    return nodes\n",
    "\n",
    "nodes = load_nodes(1, os.path.abspath('./.save'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.insert(0, os.path.abspath('.'))\n",
    "from custom.io import load_nodes_jsonl\n",
    "\n",
    "def load_nodes(pid_num, cache_dir):\n",
    "    print(\"Loading nodes\")\n",
    "    file_name = f\"gpt-4o-batch-all-p_2_parser_ManuallyHierarchicalNodeParser_8165_gpu_V100_nodeNum_200_pid_{pid_num}.jsonl\"\n",
    "    file_path = os.path.join(cache_dir, file_name)\n",
    "    nodes = load_nodes_jsonl(file_path)\n",
    "    return nodes\n",
    "\n",
    "nodes = load_nodes(1, os.path.abspath('./.save'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(df_pca['PCA1'], df_pca['PCA2'], c=df_kmeans['kmean_labels'], cmap='viridis', s = 50, alpha=0.5) \n",
    "\n",
    "# Create the scatter plot\n",
    "# level2int = {level : i for i, level in enumerate(['document', 'section', 'paragraph', 'multi-sentences'])}\n",
    "# scatter = plt.scatter(df_pca['PCA1'], df_pca['PCA2'], c=[level2int[node.metadata['level']] for node in nodes], cmap='viridis', s = 50, alpha=0.5) \n",
    "# Reverse the mapping for easy label creation\n",
    "# int2level = {v: k for k, v in level2int.items()}\n",
    "\n",
    "# Set the ticks to match the integer labels for the levels\n",
    "# cbar = plt.colorbar(scatter, label='labels')\n",
    "# cbar.set_ticks([0, 1, 2, 3])\n",
    "# cbar.set_ticklabels(['document', 'section', 'paragraph', 'multi-sentences'])\n",
    "\n",
    "# Create a legend using custom patches\n",
    "# handles = [mpatches.Patch(color=scatter.cmap(scatter.norm(level2int[level])), label=level) \n",
    "#            for level in level2int]\n",
    "unique_labels = np.unique(df_kmeans['kmean_labels'])\n",
    "# handles = [mpatches.Patch(color=scatter.cmap(scatter.norm(level2int[level])), label=level) \n",
    "#            for level in level2int]\n",
    "handles = [mpatches.Patch(color=scatter.cmap(scatter.norm(label)), label=f'Label {label}') for label in unique_labels]\n",
    "\n",
    "# Add the legend to the plot (in the upper right corner)\n",
    "plt.legend(handles=handles, title=\"Labels\", loc='upper right')\n",
    "\n",
    "\n",
    "# Add labels to the points\n",
    "# for i, node in enumerate(nodes):\n",
    "#     label = node.metadata['level']  # Get the text label\n",
    "#     plt.text(df_pca['PCA1'].iloc[i], df_pca['PCA2'].iloc[i], label, fontsize=8, ha='right', color='black')\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('PCA of Clustered Embeddings')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_node = [node.metadata['level'] in ['document', 'section', 'paragraph', 'multi-sentences'] for node in nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 8))\n",
    "# scatter = plt.scatter(df_ae['AE1'], df_ae['AE2'], c=df_kmeans['kmean_labels'], cmap='viridis', s=50)\n",
    "\n",
    "# Create the scatter plot\n",
    "level2int = {level : i for i, level in enumerate(['document', 'section', 'paragraph', 'multi-sentences'])}\n",
    "scatter = plt.scatter(df_ae['AE1'][selected_node], df_ae['AE2'][selected_node], c=[level2int[node.metadata['level']] for i, node in enumerate(nodes) if selected_node[i]], cmap='viridis', s = 50, alpha=0.5) \n",
    "# Reverse the mapping for easy label creation\n",
    "int2level = {v: k for k, v in level2int.items()}\n",
    "\n",
    "# Create a legend using custom patches\n",
    "handles = [mpatches.Patch(color=scatter.cmap(scatter.norm(level2int[level])), label=level) \n",
    "           for level in level2int]\n",
    "\n",
    "# Add the legend to the plot (in the upper right corner)\n",
    "plt.legend(handles=handles, title=\"Labels\", loc='upper right')\n",
    "\n",
    "plt.title('AutoEncoder of Clustered Embeddings')\n",
    "plt.xlabel('AutoEncoder Component 1')\n",
    "plt.ylabel('AutoEncoder Component 2')\n",
    "plt.show()\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 8))\n",
    "# scatter = plt.scatter(df_pca['PCA1'], df_pca['PCA2'], c=df_kmeans['kmean_labels'], cmap='viridis', s = 50, alpha=0.5) \n",
    "\n",
    "\n",
    "# Set the ticks to match the integer labels for the levels\n",
    "# cbar = plt.colorbar(scatter, label='labels')\n",
    "# cbar.set_ticks([0, 1, 2, 3])\n",
    "# cbar.set_ticklabels(['document', 'section', 'paragraph', 'multi-sentences'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
