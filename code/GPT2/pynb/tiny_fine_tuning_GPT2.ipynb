{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ItYpaZD9EH7J",
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "\n",
        "from utils import read_env\n",
        "from PyPDF2 import PdfReader\n",
        "import os\n",
        "import docx\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### proxy service"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
        "output = result.stdout\n",
        "for line in output.splitlines():\n",
        "    if '=' in line:\n",
        "        var, value = line.split('=', 1)\n",
        "        os.environ[var] = value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Ly_QfYPDHlie",
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [],
      "source": [
        "# Functions to read different file types\n",
        "def read_pdf(file_path):\n",
        "    with open(file_path, \"rb\") as file:\n",
        "        pdf_reader = PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page_num in range(len(pdf_reader.pages)):\n",
        "            text += pdf_reader.pages[page_num].extract_text()\n",
        "    return text\n",
        "\n",
        "def read_word(file_path):\n",
        "    doc = docx.Document(file_path)\n",
        "    text = \"\"\n",
        "    for paragraph in doc.paragraphs:\n",
        "        text += paragraph.text + \"\\n\"\n",
        "    return text\n",
        "\n",
        "def read_txt(file_path):\n",
        "    with open(file_path, \"r\") as file:\n",
        "        text = file.read()\n",
        "    return text\n",
        "\n",
        "def read_documents_from_directory(directory):\n",
        "    combined_text = []\n",
        "    for i, filename in enumerate(os.listdir(directory)):\n",
        "        file_path = os.path.join(directory, filename)\n",
        "        if filename.endswith(\".pdf\"):\n",
        "            new_text = read_pdf(file_path)\n",
        "        elif filename.endswith(\".docx\"):\n",
        "            new_text = read_word(file_path)\n",
        "        elif filename.endswith(\".txt\"):\n",
        "            new_text = read_txt(file_path)\n",
        "        # new_text = re.sub(r'\\n+', '\\n', new_text).strip()\n",
        "        combined_text.append(new_text)\n",
        "        if i == 9:\n",
        "            break\n",
        "    return combined_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "oobynnecHx87",
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [],
      "source": [
        "# Load documents from the directory\n",
        "data_path = os.getenv(\"data_path\")\n",
        "all_text = read_documents_from_directory(os.path.join(data_path, \"ori_papers\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZaBPMO-GZQP",
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [],
      "source": [
        "# Save the training and validation data as text files\n",
        "train_vs_test = [7, 3]\n",
        "train_id = int(len(all_text)/10*7)\n",
        "\n",
        "train_text = \"\".join(all_text[:train_id])\n",
        "test_text = \"\".join(all_text[train_id:])\n",
        "\n",
        "if os.path.exists(os.path.join(data_path, \"dataset\")):\n",
        "   shutil.rmtree(os.path.join(data_path, \"dataset\"))\n",
        "os.makedirs(os.path.join(data_path, \"dataset\"))\n",
        "\n",
        "with open(os.path.join(data_path, \"dataset/train.txt\"), \"w\") as f:\n",
        "   f.write(train_text)\n",
        "with open(os.path.join(data_path, \"dataset/test.txt\"), \"w\") as f:\n",
        "   f.write(test_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "9MFaNgaDEVKP",
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [],
      "source": [
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Wk_14dI9EVdD",
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [],
      "source": [
        "def load_dataset(file_path, tokenizer, block_size = 128):\n",
        "    dataset = TextDataset(\n",
        "        tokenizer = tokenizer,\n",
        "        file_path = file_path,\n",
        "        block_size = block_size,\n",
        "    )\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QJcoT-aNFcKp",
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [],
      "source": [
        "def load_data_collator(tokenizer, mlm = False):\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, \n",
        "        mlm=mlm,\n",
        "    )\n",
        "    return data_collator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    \n",
        "    return {\"accuracy\": accuracy}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [],
      "source": [
        "def train(train_file_path, model_name, \n",
        "          output_dir, \n",
        "          overwrite_output_dir, \n",
        "          per_device_train_batch_size, \n",
        "          num_train_epochs,\n",
        "          test_file_path=None,\n",
        "          save_strategy=\"no\",\n",
        "          save_steps=0,\n",
        "          logging_steps=500):\n",
        "    \n",
        "    if save_steps != None: save_strategy = \"steps\"\n",
        "    \n",
        "    if os.path.exists(output_dir):\n",
        "        shutil.rmtree(output_dir)\n",
        "    os.makedirs(output_dir)\n",
        "    os.makedirs(os.path.join(output_dir, \"tokenizer\"))\n",
        "\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "    train_dataset = load_dataset(train_file_path, tokenizer)\n",
        "    \n",
        "    test_dataset = None\n",
        "    if test_file_path != None:\n",
        "        test_dataset = load_dataset(test_file_path, tokenizer)\n",
        "    \n",
        "    data_collator = load_data_collator(tokenizer)\n",
        "    tokenizer.save_pretrained(os.path.join(output_dir, 'tokenizer'))\n",
        "\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model.to(device)\n",
        "    model.save_pretrained(output_dir)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "            overwrite_output_dir=overwrite_output_dir,\n",
        "            per_device_train_batch_size=per_device_train_batch_size,\n",
        "            num_train_epochs=num_train_epochs,\n",
        "            save_strategy=save_strategy,\n",
        "            save_steps=save_steps,\n",
        "            logging_steps=logging_steps,\n",
        "            include_inputs_for_metrics=True,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            compute_metrics=compute_metrics,\n",
        "            data_collator=data_collator,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=test_dataset,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    trainer.save_model(os.path.join(output_dir, \"final\"))\n",
        "    return trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9mXiWKHbFr2f",
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [],
      "source": [
        "train_file_path = os.path.join(data_path, \"dataset/train.txt\")\n",
        "model_name = 'gpt2'\n",
        "output_dir = os.getenv(\"output_dir\")\n",
        "overwrite_output_dir = False\n",
        "per_device_train_batch_size = 8\n",
        "num_train_epochs = 50.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "WMdaTo7KF9uo",
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/Fine-Tuned-GPT-2-with-articles-ground-truth/.conda/lib/python3.11/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n",
            "/root/Fine-Tuned-GPT-2-with-articles-ground-truth/.conda/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7900' max='7900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7900/7900 06:05, Epoch 50/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>2.538700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.903200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>1.540600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>1.255200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>1.025500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.840400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.696400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.584700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.506400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.443900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.391700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.355600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.326200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.306900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.292800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Train\n",
        "trainer = train(\n",
        "    train_file_path=train_file_path,\n",
        "    model_name=model_name,\n",
        "    output_dir=output_dir,\n",
        "    overwrite_output_dir=overwrite_output_dir,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    test_file_path=os.path.join(data_path, \"dataset/test.txt\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [40/40 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "ValueError",
          "evalue": "multiclass-multioutput is not supported",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Fine-Tuned-GPT-2-with-articles-ground-truth/.conda/lib/python3.11/site-packages/transformers/trainer.py:3043\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3040\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3042\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3043\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3044\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3045\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3046\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3047\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3048\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3051\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3053\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3054\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
            "File \u001b[0;32m~/Fine-Tuned-GPT-2-with-articles-ground-truth/.conda/lib/python3.11/site-packages/transformers/trainer.py:3339\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m all_preds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m all_labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_inputs_for_metrics:\n\u001b[0;32m-> 3339\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3340\u001b[0m \u001b[43m            \u001b[49m\u001b[43mEvalPrediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3341\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3342\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3343\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(EvalPrediction(predictions\u001b[38;5;241m=\u001b[39mall_preds, label_ids\u001b[38;5;241m=\u001b[39mall_labels))\n",
            "Cell \u001b[0;32mIn[4], line 4\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[0;34m(pred)\u001b[0m\n\u001b[1;32m      2\u001b[0m labels \u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39mlabel_ids\n\u001b[1;32m      3\u001b[0m preds \u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39mpredictions\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: accuracy}\n",
            "File \u001b[0;32m~/Fine-Tuned-GPT-2-with-articles-ground-truth/.conda/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
            "File \u001b[0;32m~/Fine-Tuned-GPT-2-with-articles-ground-truth/.conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:213\u001b[0m, in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[0;32m--> 213\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[0;32m~/Fine-Tuned-GPT-2-with-articles-ground-truth/.conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:105\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# No metrics support \"multiclass-multioutput\" format\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-indicator\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(y_type))\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    108\u001b[0m     xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred)\n",
            "\u001b[0;31mValueError\u001b[0m: multiclass-multioutput is not supported"
          ]
        }
      ],
      "source": [
        "trainer.evaluate()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwGS1IMlGBMB"
      },
      "source": [
        "Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lelq_sN4Gy5M"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GenerationConfig\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "wOvrNQRAG2IP"
      },
      "outputs": [],
      "source": [
        "def load_model(model_path):\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_tokenizer(model_path):\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(os.path.join(model_path, \"tokenizer\"))\n",
        "    return tokenizer\n",
        "\n",
        "def evaluate_perplexity(model, tokenizer, generated_sequence):\n",
        "    # Tokenize the generated sequence\n",
        "    input_ids = tokenizer.encode(generated_sequence, return_tensors='pt')\n",
        "\n",
        "    # Get model logits for the generated sequence\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_ids)[0]\n",
        "\n",
        "    # Calculate perplexity\n",
        "    perplexity = torch.exp(torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), input_ids.view(-1)))\n",
        "\n",
        "    return perplexity.item()\n",
        "\n",
        "def generate_text(model_path, sequence, max_length, model=None, verbose=False):\n",
        "    if (model == None):\n",
        "        model = load_model(model_path)\n",
        "    tokenizer = load_tokenizer(model_path)\n",
        "    ids = tokenizer.encode(f'{sequence}', return_tensors='pt')\n",
        "    generation_config = GenerationConfig(\n",
        "        max_new_tokens=max_length,\n",
        "        early_stopping=True,\n",
        "        num_beams = 3,\n",
        "        top_k=10,\n",
        "        top_p=0.80,\n",
        "        remove_invalid_values = True,\n",
        "        pad_token_id = model.config.eos_token_id,\n",
        "        eos_token_id = model.config.eos_token_id,\n",
        "    )\n",
        "\n",
        "    final_outputs = model.generate(\n",
        "        inputs=ids,\n",
        "        generation_config = generation_config,\n",
        "        return_dict_in_generate=True, \n",
        "        output_scores=True\n",
        "    )\n",
        "\n",
        "    transition_scores = model.compute_transition_scores(\n",
        "        final_outputs.sequences, final_outputs.scores, normalize_logits=True\n",
        "    )[0]\n",
        "\n",
        "    generated_tokens = final_outputs.sequences[0]\n",
        "    if verbose:\n",
        "        for tok, score in zip(generated_tokens, transition_scores):\n",
        "            # | token | token string | log probability | probability\n",
        "            print(f\"| {tok:5d} | {tokenizer.decode(tok):15s} | {score.numpy():6.2f} | {np.exp(score.numpy()):.2%}\")\n",
        "\n",
        "    return tokenizer.decode(generated_tokens, skip_special_tokens=True)[len(sequence)+1:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvNx7gjeRieD"
      },
      "source": [
        "This model got trained on the entire text and took much longer to train, and yet it fails to give meaningful results. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'GPT2LMHeadModel' object has no attribute 'evaluate'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m load_model(output_dir)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m()\n",
            "File \u001b[0;32m~/Fine-Tuned-GPT-2-with-articles-ground-truth/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'GPT2LMHeadModel' object has no attribute 'evaluate'"
          ]
        }
      ],
      "source": [
        "model = load_model(output_dir)\n",
        "model.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mTDTrpnG5Ut",
        "outputId": "e37387f1-beb6-4fa6-d5fd-ec2dfd775978"
      },
      "outputs": [],
      "source": [
        "model1_path = output_dir\n",
        "sequence1 = \"[Q] What is the Monosaccharide?\"\n",
        "max_len = 100\n",
        "print(generate_text(model1_path, sequence1, max_len))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1_path = output_dir\n",
        "sequence1 = \"[Q] Please generate 100 questions about Monosaccharide\"\n",
        "max_len = 100\n",
        "print(generate_text(model1_path, sequence1, max_len)[len(sequence1)+1:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/Fine-Tuned-GPT-2-with-articles-ground-truth/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "import evaluate\n",
        "rouge = evaluate.load('rouge')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load reference and candidate responses from text files\n",
        "def load_file(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        file = [line.strip() for line in file]\n",
        "    return file\n",
        "\n",
        "def generate_test_answer_candidate(question_path, answer_path):\n",
        "    questions = load_file(question_path)\n",
        "    answers = []\n",
        "    for q in questions:\n",
        "        answer = generate_text(model1_path, sequence1, max_len)\n",
        "        answers.append(answer)\n",
        "    with open(answer_path, \"w\") as f:\n",
        "        f.write(\"\\n\".join(answers))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generate_test_answer_candidate(os.getenv(\"question_path\"), os.getenv(\"candidate_answers_path\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reference_responses = load_file(os.getenv(\"reference_answers_path\"))\n",
        "candidate_responses = load_file(os.getenv(\"candidate_answers_path\"))\n",
        "\n",
        "# Tokenization (assuming responses are already tokenized)\n",
        "reference_tokenized = [response.split() for response in reference_responses]\n",
        "candidate_tokenized = [response.split() for response in candidate_responses]\n",
        "\n",
        "# Calculate BLEU score\n",
        "bleu_score = corpus_bleu(reference_tokenized, candidate_tokenized, smoothing_function=SmoothingFunction().method1)\n",
        "\n",
        "print(\"BLEU Score:\", bleu_score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate ROUGE score\n",
        "result = rouge.compute(predictions=candidate_responses, references=reference_responses)\n",
        "\n",
        "print(\"Rouge\\n    score1: {}, score2: {}\\n    rougeL: {}, rougeLsum: {}\".format(result[\"rouge1\"], result[\"rouge2\"], result[\"rougeL\"], result[\"rougeLsum\"]))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNvBu4Jj2F8tAESALuZqmkV",
      "include_colab_link": true,
      "mount_file_id": "1WRB2uz0Mpfkw6AnDAxCOPwY_TT2tRizG",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
