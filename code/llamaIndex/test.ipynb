{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.environ.get('OPENAI_API_KEY'))\n",
    "\n",
    "# batch_input_file = client.files.create(\n",
    "#   file=open(\"D:\\\\Projects(D)\\\\Fine-Tuned-GPT-2-with-articles-ground-truth\\\\code\\\\llamaIndex\\\\.cache\\\\batchinput.jsonl\", \"rb\"),\n",
    "#   purpose=\"batch\"\n",
    "# )\n",
    "\n",
    "# batch_input_file_id = batch_input_file.id\n",
    "\n",
    "# batch = client.batches.create(\n",
    "#     input_file_id=batch_input_file_id,\n",
    "#     endpoint=\"/v1/chat/completions\",\n",
    "#     completion_window=\"24h\",\n",
    "#     metadata={\n",
    "#       \"description\": \"nightly eval job\"\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.batches.retrieve('batch_5TFUPEN2etKNpSWnq4Kh5fi4').__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = client.files.content('file-3DR1YeMpP7JHGpHWDwXzsh17')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(content.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output_file.jsonl', 'w') as output_file:\n",
    "    output_file.write(content.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "content = json.loads(content.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from database import Database\n",
    "\n",
    "d = Database(config_dir_path='./code/llamaIndex')\n",
    "nodes = d.create_or_update_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "client = OpenAI(api_key=os.environ.get('OPENAI_API_KEY'))\n",
    "root_path = '../..'\n",
    "cache_dir = os.path.abspath(os.path.join(root_path, './code/llamaIndex/.cache'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\\\n",
    "You are a highly knowledgeable reasearch assistant tasked with generating insightful questions, detailed answers, and \\\n",
    "thorough reasoning based on the provided parts of papers.\\\n",
    "\"\"\"\n",
    "\n",
    "prompt_template_openai=\"\"\"\\\n",
    "Here is the context:\n",
    "{context_str}\n",
    "\n",
    "Using this context, generate 5 specific questions that this context can uniquely answer. Ensure that these questions:\n",
    "1. Are directly related to the provided context.\n",
    "2. Highlight unique information or insights from the context.\n",
    "3. Cannot be easily answered by general knowledge.\n",
    "----------------------------------------------------------------------------------\n",
    "<Pair number, representing which QAR you are at, like 1, 2, 3>\n",
    "Question:<Question content, you should place a specific question which is unlikely to be found elsewhere and is unique comparing with other questions>\n",
    "\n",
    "Answer:<Answer content, you should place a specific answer combining with the offered context>\n",
    "\n",
    "Reason:<Reason content, you should explain why this question and answer are unlikely to be found elsewhere and are unique comparing with each other>\n",
    "----------------------------------------------------------------------------------\n",
    "Higher-level summaries of surrounding context may be provided \\\n",
    "as well. Try using these summaries to generate better questions \\\n",
    "that this context can answer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch(now, file_path):\n",
    "    batch_input_file = client.files.create(\n",
    "        file=open(file_path, \"rb\"),\n",
    "        purpose=\"batch\"\n",
    "    )\n",
    "\n",
    "    batch_input_file_id = batch_input_file.id\n",
    "\n",
    "    batch = client.batches.create(\n",
    "        input_file_id=batch_input_file_id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\"\n",
    "    )\n",
    "    batch_path = os.path.join(cache_dir, f\"{now}-batchinput-response.json\")\n",
    "    with open(batch_path, 'w') as file:\n",
    "        json.dump(batch.to_dict(), file, indent=4)\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "batches = []\n",
    "node_id = 0\n",
    "for node in nodes[:2]:\n",
    "    if node_id % 50000 == 0:\n",
    "        now = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "        file_path = os.path.join(cache_dir, f\"{now}-batchinput.jsonl\")\n",
    "        file = open(file_path, 'w+')\n",
    "    entry = {\n",
    "            \"custom_id\": node.id_,\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": \"gpt-4o\",\n",
    "                \"messages\": [\n",
    "                        {\"role\": \"system\", \"content\": system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": prompt_template_openai.format(context_str=node.text)}\n",
    "                    ],\n",
    "                \"max_tokens\": 4096\n",
    "            }\n",
    "        }\n",
    "    json_line = json.dumps(entry)\n",
    "    file.write(json_line + \"\\n\")\n",
    "    if (node_id+1) % 50000 == 0:\n",
    "        file.close()\n",
    "        batch = create_batch(now, file_path)\n",
    "        batches.append(batch)\n",
    "    node_id += 1\n",
    "if (node_id+1) % 50000 != 0:\n",
    "    file.close()\n",
    "    batch = create_batch(now, file_path)\n",
    "    batches.append(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input_file = client.files.create(\n",
    "    file=open(file_path, \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input_file_id = batch_input_file.id\n",
    "\n",
    "batch = client.batches.create(\n",
    "    input_file_id=batch_input_file_id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\"\n",
    ")\n",
    "batch_path = os.path.join(cache_dir, f\"{now}-batchinput-response.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches.append(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = [batches[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finished_contents = set()\n",
    "while len(finished_contents) < len(batches):\n",
    "    # Refresh the batch response\n",
    "    for batch in batches:\n",
    "        batch = client.batches.retrieve(batch.id)\n",
    "        if batch.status == 'completed':\n",
    "            if batch.request_counts.completed == batch.request_counts.total:\n",
    "                content = client.files.content(batch.output_file_id)\n",
    "                finished_contents.add(content)\n",
    "            else:\n",
    "                print(f\"Get {batch.status} at batch {batch.id}\")\n",
    "                exit()\n",
    "        if batch.status in ['expired', 'failed', 'cancelled']:\n",
    "            print(f\"Get {batch.status} at batch {batch.id}\")\n",
    "            exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = client.batches.retrieve(batch.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.request_counts.completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.files.content(batch.error_file_id).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =[json.loads(response) for response in content.text.strip().split('\\n')]\n",
    "with open('output_file.json', 'w') as output_file:\n",
    "    json.dump(data, output_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for request in data:\n",
    "    print(request['response']['body']['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_dict = {}\n",
    "for node in nodes[:2]:\n",
    "    print(node.id_)\n",
    "    node_dict[node.id_] = node\n",
    "    print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for request in data:\n",
    "    node_dict[request['custom_id']].metadata[\"questions_this_excerpt_can_answer_and_corresponding_answers_and_reasons\"] = request['response']['body']['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in nodes:\n",
    "    print(node.get_content(metadata_mode='embed'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 10/10 [00:10<00:00,  1.00s/batch]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "uncompleted_batches = [random.randint(1, 100) for _ in range(10)]\n",
    "total_batches = len(uncompleted_batches)\n",
    "\n",
    "with tqdm(total=total_batches, desc=\"Processing batches\", unit=\"batch\") as pbar:\n",
    "    while len(uncompleted_batches) > 0:\n",
    "        for _ in range(2):  # 10-minute countdown\n",
    "            # Simulate batch completion\n",
    "            uncompleted_batches.pop()\n",
    "\n",
    "        # Update the progress bar\n",
    "        pbar.n = total_batches - len(uncompleted_batches)\n",
    "        pbar.refresh()\n",
    "        time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_batches = int(len(nodes)/45000) + (len(nodes) % 45000 > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[179], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m extractor_index_config \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      2\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o-QAExtractor-batch-interrupted\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneed_interrupt\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforce_extract\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     }}\n\u001b[0;32m      6\u001b[0m ]\n\u001b[1;32m----> 7\u001b[0m (extractor_name, extractor_config) \u001b[38;5;241m=\u001b[39m extractor_index_config[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(extractor_name)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(extractor_config)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "extractor_index_config = {\n",
    "    \"gpt-4o-QAExtractor-batch-interrupted\": {\n",
    "        \"need_interrupt\": True,\n",
    "        \"force_extract\": True\n",
    "    }\n",
    "}\n",
    "extractor_name, extractor_config = next(iter(extractor_index_config.items()))\n",
    "\n",
    "print(extractor_name)\n",
    "print(extractor_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
